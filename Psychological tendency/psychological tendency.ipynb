{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test1 LGBM and ExtraTrees with or without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/fds/Dev/Python/2022_AI_competition/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.read_csv(path+'train.csv')\n",
    "test  = pd.read_csv(path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_list = ['Q1','Q2','Q3','Q4','Q5', 'Q6','Q7','Q8','Q9',\n",
    "#             'Q10','Q11','Q12','Q13','Q14','Q15', 'Q16','Q17','Q18','Q19',\n",
    "#             'Q20','Q21','Q22','Q23','Q24','Q25', 'Q26',\n",
    "#             'index', 'country', 'introelapse','testelapse','surveyelapse']\n",
    "drop_list = ['index', 'country', ]\n",
    "# replace_dict = {'education': str, 'married': str}\n",
    "replace_dict = {'education': str, 'engnat': str, 'married': str, 'urban': str}\n",
    "# replace_dict = {'education': str, 'engnat': str, 'urban': str}\n",
    "train_data =pd.read_csv(path+'train.csv')\n",
    "hold_out = int(len(train_data)*0.8)\n",
    "test_data = pd.read_csv(path+'test.csv')\n",
    "\n",
    "# train_data = train_data.drop(train_data[train_data.familysize > 11].index)\n",
    "\n",
    "# train = train_data.drop(drop_list, axis=1)\n",
    "train_y = train_data['nerdiness']\n",
    "train_x = train_data.drop(drop_list + ['nerdiness'], axis=1)\n",
    "test_x = test_data.drop(drop_list, axis=1)\n",
    "\n",
    "train_x = train_x.reset_index(drop=True)\n",
    "train_y = train_y.reset_index(drop=True)\n",
    "\n",
    "\n",
    "columns = train_x.columns.values\n",
    "print(columns)\n",
    "\n",
    "train_x = pd.get_dummies(train_x)\n",
    "test_x = pd.get_dummies(test_x)\n",
    "\n",
    "imp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp_mode.fit(train_x)\n",
    "train_x = imp_mode.transform(train_x)\n",
    "test_x = imp_mode.transform(test_x)\n",
    "\n",
    "# scaler = RobustScaler()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(train_x)\n",
    "X_test_scaled = scaler.transform(test_x)\n",
    "\n",
    "# scaler.fit(train_x)\n",
    "# X_train_scaled = scaler.transform(train_x)\n",
    "# X_test_scaled = scaler.transform(test_x)\n",
    "\n",
    "# train = np.nan_to_num(train)\n",
    "# test = np.nan_to_num(test)\n",
    "\n",
    "# X_val_scaled = X_train_scaled[hold_out:]\n",
    "# X_train_scaled = X_train_scaled[:hold_out]\n",
    "# y_val = train_y[hold_out:]\n",
    "# y_train = train_y[:hold_out]\n",
    "\n",
    "# val = train[hold_out:]\n",
    "# train = train[:hold_out]\n",
    "\n",
    "# X_train_scaled, train_y_t = torch.tensor(X_train_scaled.reshape(-1, 2, 200)).to(DEVICE), torch.tensor(train_y).to(DEVICE)  \n",
    "# X_test_scaled = torch.tensor(X_train_scaled.reshape(-1, 2, 200)).to(DEVICE)\n",
    "pca = PCA(n_components=62) # 주성분을 몇개로 할지 결정\n",
    "X_train_scaled_printcipalComponents = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled_printcipalComponents = pca.transform(X_test_scaled)\n",
    "\n",
    "X_val_scaled = X_train_scaled_printcipalComponents[hold_out:]\n",
    "X_train_scaled = X_train_scaled_printcipalComponents[:hold_out]\n",
    "y_val = train_y[hold_out:]\n",
    "y_train = train_y[:hold_out]\n",
    "\n",
    "# principalDf = pd.DataFrame(data=printcipalComponents, columns = columns[0:62])\n",
    "print(X_train_scaled_printcipalComponents.shape)\n",
    "\n",
    "# train_y_t = torch.tensor(train_y, dtype=torch.float32)\n",
    "# X_train_scaled = torch.tensor(X_train_scaled_printcipalComponents, dtype=torch.float32)\n",
    "# X_test_scaled = torch.tensor(X_test_scaled_printcipalComponents, dtype=torch.float32)\n",
    "\n",
    "test_len = len(test_x)\n",
    "\n",
    "N_REPEAT = 5\n",
    "N_SKFOLD = 7\n",
    "N_EPOCH = 300\n",
    "BATCH_SIZE = 2048\n",
    "VIRTUAL_BATCH_SIZE = 1024\n",
    "LOADER_PARAM = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True\n",
    "}\n",
    "\n",
    "prediction = np.zeros((test_len, 1), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['index', 'country'],axis = 1)\n",
    "test = test.drop(['index', 'country'],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train을 target과 feature로 나눠줍니다.\n",
    "train_x=train.drop(['nerdiness'], axis=1)\n",
    "train_y=train['nerdiness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp_mode.fit(train_x)\n",
    "train_x = imp_mode.transform(train_x)\n",
    "test_x = imp_mode.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.33, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x == np.NaN).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_clf = LGBMClassifier(\n",
    "#             n_estimators=1000\n",
    "#         )\n",
    "# lgbm_clf.fit(X_train, y_train)\n",
    "\n",
    "# lgbm_pred = lgbm_clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clf = ExtraTreesClassifier(n_estimators=20000, criterion=\"entropy\",max_depth=None, min_samples_split=10, random_state=123, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_val, et_clf.predict_proba(X_val_scaled)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = et_clf.predict_proba(X_val)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(et_clf, train_x, train_y, scoring = 'accuracy', cv =5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = et_clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(path+'sample_submission.csv')\n",
    "submission[\"nerdiness\"] = proba\n",
    "submission.to_csv(\"submission.csv\", index = False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test2 AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.read_csv(path+'train.csv')\n",
    "test  = pd.read_csv(path+'test.csv')\n",
    "train = train.drop(['index', 'country'],axis = 1)\n",
    "test = test.drop(['index', 'country'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup(Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = setup(data=train, target='nerdiness', \n",
    "            feature_selection = True,\n",
    "            pca = True, pca_components = 10,\n",
    "            imputation_type='iterative',\n",
    "            remove_multicollinearity = True,\n",
    "            multicollinearity_threshold = 0.95, \n",
    "            fix_imbalance = True,\n",
    "            fold_strategy = 'stratifiedkfold', fold = 10, fold_shuffle=True, \n",
    "            session_id = 123, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_5_l = compare_models(include=['et','rf','xgboost','lightgbm'], sort='AUC', n_select=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended = blend_models(estimator_list = best_5_l, fold = 10, method = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_holdout = predict_model(blended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = finalize_model(blended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_model(final_model, data = test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(path+'sample_submission.csv')\n",
    "submission[\"nerdiness\"] = predictions['Label']\n",
    "submission[\"nerdiness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_aml_label.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = pd.read_csv('/home/fds/Dev/Python/2022_AI_competition/data/submission_inverse_conv1d_pred_fold7_1.csv')\n",
    "compare1 = pd.read_csv('/home/fds/Dev/Python/2022_AI_competition/submission_model_PCA_inverse_deeper2_repeat3_fold6_0.0_load.csv')\n",
    "compare2 = pd.read_csv('/home/fds/Dev/Python/2022_AI_competition/submission_model_PCA_inverse_deeper2_repeat4_fold6_0.0_load.csv')\n",
    "compare3 = pd.read_csv('/home/fds/Dev/Python/2022_AI_competition/submission_model_PCA_inverse_deeper2_repeat2_fold6_2.0146200902141807e-10_load.csv')\n",
    "lastoutput1 = pd.read_csv('/home/fds/Dev/Python/2022_AI_competition/data/submission_model_PCA_inverse_deeper2_conv1d_pred_fold7_go_0?_.csv')\n",
    "lastoutput2 = pd.read_csv('/home/fds/Dev/Python/2022_AI_competition/data/submission_model_PCA_inverse_deeper2_conv1d_pred_fold7_go_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = best['nerdiness'] > 0\n",
    "compare1 = compare1['nerdiness'] > 0\n",
    "compare2 = compare2['nerdiness'] > 0\n",
    "compare3 = compare3['nerdiness'] > 0\n",
    "lastoutput1 = lastoutput1['nerdiness'] > 0\n",
    "lastoutput2 = lastoutput2['nerdiness'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best.compare(compare1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best.compare(lastoutput2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastoutput2.compare(lastoutput1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best.compare(compare1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test3 Neural network with conv1d (But it has mistakes in train code!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fixed seed for reproducibility\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/fds/Dev/Python/2022_AI_competition/data/'\n",
    "save_path = '/data/pytorch/2022_AI_Competition/model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['index', 'country', 'introelapse','testelapse','surveyelapse', 'hand']\n",
    "replace_dict = {'education': str, 'engnat': str, 'married': str, 'urban': str}\n",
    "train_data =pd.read_csv(path+'train.csv')\n",
    "test_data = pd.read_csv(path+'test.csv')\n",
    "\n",
    "train_data = train_data.drop(train_data[train_data.familysize > 11].index)\n",
    "\n",
    "train_y = train_data['nerdiness']\n",
    "train_x = train_data.drop(drop_list + ['nerdiness'], axis=1)\n",
    "test_x = test_data.drop(drop_list, axis=1)\n",
    "\n",
    "train_x = train_x.reset_index(drop=True)\n",
    "train_y = train_y.reset_index(drop=True)\n",
    "\n",
    "\n",
    "columns = train_x.columns.values\n",
    "\n",
    "train_x = pd.get_dummies(train_x)\n",
    "test_x = pd.get_dummies(test_x)\n",
    "\n",
    "imp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp_mode.fit(train_x)\n",
    "train_x = imp_mode.transform(train_x)\n",
    "test_x = imp_mode.transform(test_x)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(train_x)\n",
    "X_test_scaled = scaler.transform(test_x)\n",
    "\n",
    "train_y_t = torch.tensor(train_y, dtype=torch.float32)\n",
    "X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_scaled = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "test_len = len(test_x)\n",
    "\n",
    "N_REPEAT = 5\n",
    "N_SKFOLD = 7\n",
    "N_EPOCH = 100\n",
    "BATCH_SIZE = 2048\n",
    "VIRTUAL_BATCH_SIZE = 1024\n",
    "LOADER_PARAM = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True\n",
    "}\n",
    "\n",
    "prediction = np.zeros((test_len, 1), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/nawidsayed/lightgbm-and-cnn-3rd-place-solution/notebook\n",
    "# https://www.kaggle.com/code/nagiss/9-solution-nagiss-part-2-2-weight-sharing-nn/notebook\n",
    "\n",
    "# class Conv1d_with_SEBlock(nn.Module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__()\n",
    "#         pass\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super(Conv1dBlock, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.batchNorm1d = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        x = self.batchNorm1d(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SangModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SangModel, self).__init__()\n",
    "        self.hidden_size = 64\n",
    "\n",
    "        self.conv7 = Conv1dBlock(63, self.hidden_size*64)  \n",
    "        self.conv6 = Conv1dBlock(self.hidden_size*64, self.hidden_size*32)  \n",
    "        self.conv5 = Conv1dBlock(self.hidden_size*32, self.hidden_size*16)  \n",
    "        self.conv4 = Conv1dBlock(self.hidden_size*16, self.hidden_size*8)  \n",
    "        self.conv3 = Conv1dBlock(self.hidden_size*8, self.hidden_size*4)  \n",
    "        self.conv2 = Conv1dBlock(self.hidden_size*4, self.hidden_size*2)  \n",
    "        self.conv1 = Conv1dBlock(self.hidden_size*2, self.hidden_size)  \n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x_):\n",
    "        x = x_.unsqueeze(-1)\n",
    "\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x    \n",
    "\n",
    "model = SangModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for repeat in range(N_REPEAT):\n",
    "\n",
    "    skf, tot = StratifiedKFold(n_splits=N_SKFOLD, random_state=repeat, shuffle=True), 0.\n",
    "    for skfold, (train_idx, valid_idx) in enumerate(skf.split(X_train_scaled, train_y_t)):\n",
    "        train_idx, valid_idx = list(train_idx), list(valid_idx)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(X_train_scaled[train_idx, :], train_y_t[train_idx]),\n",
    "                                  shuffle=True, drop_last=True, **LOADER_PARAM)\n",
    "        valid_loader = DataLoader(TensorDataset(X_train_scaled[valid_idx, :], train_y_t[valid_idx]),\n",
    "                                  shuffle=False, drop_last=False, **LOADER_PARAM)\n",
    "        test_loader = DataLoader(TensorDataset(X_test_scaled, torch.zeros((test_len,), dtype=torch.float32)),\n",
    "                                 shuffle=False, drop_last=False, **LOADER_PARAM)\n",
    "        \n",
    "        # model = Model() # Problem! We miss that code.  \n",
    "        model.to(DEVICE)\n",
    "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.20665], device=DEVICE))\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=7.8e-2)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "\n",
    "        prediction_t, loss_t, acc_t = np.zeros((test_len, 1), dtype=np.float32), 1., 0.\n",
    "\n",
    "        # for epoch in range(N_EPOCH):\n",
    "        for epoch in tqdm(range(N_EPOCH), desc='{:02d}/{:02d}'.format(skfold + 1, N_SKFOLD)):\n",
    "            model.train()\n",
    "            train_running_acc, train_running_loss, train_running_count= 0, 0., 0\n",
    "            for idx, (xx, yy) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                xx, yy = xx.to(DEVICE), yy.to(DEVICE)\n",
    "                pred = model(xx).squeeze()\n",
    "                loss = criterion(pred, yy)\n",
    "                train_running_loss += loss.item() * len(yy)\n",
    "                train_running_count += len(yy)\n",
    "                train_running_acc += ((torch.sigmoid(pred) > 0.5).float() == yy).sum().item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Train loss: {train_running_loss / train_running_count}\")\n",
    "                print(f\"Train accuracy: {train_running_acc / train_running_count}\")\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                running_acc, running_loss, running_count = 0, 0., 0\n",
    "                for xx, yy in valid_loader:\n",
    "                    xx, yy = xx.to(DEVICE), yy.to(DEVICE)\n",
    "                    pred = model(xx).squeeze()\n",
    "                    loss = criterion(pred, yy)\n",
    "                    running_loss += loss.item() * len(yy)\n",
    "                    running_count += len(yy)\n",
    "                    running_acc += ((torch.sigmoid(pred) > 0.5).float() == yy).sum().item()\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"Valid loss: {running_loss / running_count}\")\n",
    "                    print(f\"Valid accuracy: {running_acc / running_count}\")\n",
    "                # if running_acc / running_count > acc_t:\n",
    "                if running_loss / running_count < loss_t:\n",
    "                    loss_t = running_loss / running_count\n",
    "                    acc_t = running_acc / running_count\n",
    "                    torch.save(model.state_dict(), f\"{save_path}model_inverse_deep1_repeat{repeat}_fold{skfold}_{loss_t}.pth\")\n",
    "                    for idx, (xx, _) in enumerate(test_loader):\n",
    "                        xx = xx.to(DEVICE)\n",
    "                        # pred = (torch.sigmoid(model(xx).detach().to('cpu'))).numpy()\n",
    "                        pred = model(xx).detach().to('cpu').numpy()\n",
    "                        prediction_t[BATCH_SIZE * idx:min(BATCH_SIZE * (idx + 1), len(prediction)), :] \\\n",
    "                            = pred[:, :].copy()\n",
    "        prediction[:, :] += prediction_t[:, :].copy() / (N_REPEAT * N_SKFOLD)\n",
    "        tot += loss_t\n",
    "    print('R{} -> {:6.10f}'.format(repeat + 1, tot / N_SKFOLD))\n",
    "\n",
    "df = pd.read_csv(path+'sample_submission.csv')\n",
    "df.iloc[:, 1:] = prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using prediction in train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path+'model_inverse_batchNorm_sigmoid_repeat_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SangModel()\n",
    "model.load_state_dict(torch.load(save_path+\"model_inverse_deeper1_1_repeat0_fold2_3.762843291348371e-06.pth\"))\n",
    "# model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_scaled = X_test_scaled.to('cpu')\n",
    "# pred = model(X_test_scaled).detach().cpu().numpy()\n",
    "pred = model(X_test_scaled).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(path+'sample_submission.csv')\n",
    "submission[\"nerdiness\"] = pred\n",
    "submission.to_csv(\"submission_model_inverse_deeper1_1_repeat0_fold2_3.762843291348371e-06.csv\", index = False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Although trian function mistakes lead to overfitting, it made the model reach higher AUC scores. (I judged that lowest loss was a best model to make high AUC scores. Thank to loss metrics, I got a best model of all my model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test4 Neural network with fixed train function mistakes (After competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fixed seed for reproducibility\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/fds/Dev/Python/2022_AI_competition/data/'\n",
    "save_path = '/data/pytorch/2022_AI_Competition/model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic\n",
    " (If you want to use PCA data, do not execute codes under this line.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_list = ['Q1','Q2','Q3','Q4','Q5', 'Q6','Q7','Q8','Q9',\n",
    "#             'Q10','Q11','Q12','Q13','Q14','Q15', 'Q16','Q17','Q18','Q19',\n",
    "#             'Q20','Q21','Q22','Q23','Q24','Q25', 'Q26',\n",
    "#             'index', 'country', 'introelapse','testelapse','surveyelapse']\n",
    "drop_list = ['index', 'country', 'introelapse','testelapse','surveyelapse', 'hand']\n",
    "# replace_dict = {'education': str, 'married': str}\n",
    "replace_dict = {'education': str, 'engnat': str, 'married': str, 'urban': str}\n",
    "train_data =pd.read_csv(path+'train.csv')\n",
    "hold_out = int(len(train_data)*0.8)\n",
    "test_data = pd.read_csv(path+'test.csv')\n",
    "\n",
    "train_data = train_data.drop(train_data[train_data.familysize > 11].index)\n",
    "\n",
    "# train = train_data.drop(drop_list, axis=1)\n",
    "train_y = train_data['nerdiness']\n",
    "train_x = train_data.drop(drop_list + ['nerdiness'], axis=1)\n",
    "test_x = test_data.drop(drop_list, axis=1)\n",
    "\n",
    "train_x = train_x.reset_index(drop=True)\n",
    "train_y = train_y.reset_index(drop=True)\n",
    "\n",
    "\n",
    "columns = train_x.columns.values\n",
    "print(columns)\n",
    "\n",
    "train_x = pd.get_dummies(train_x)\n",
    "test_x = pd.get_dummies(test_x)\n",
    "\n",
    "imp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp_mode.fit(train_x)\n",
    "train_x = imp_mode.transform(train_x)\n",
    "test_x = imp_mode.transform(test_x)\n",
    "\n",
    "# scaler = RobustScaler()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(train_x)\n",
    "X_test_scaled = scaler.transform(test_x)\n",
    "\n",
    "# scaler.fit(train_x)\n",
    "# X_train_scaled = scaler.transform(train_x)\n",
    "# X_test_scaled = scaler.transform(test_x)\n",
    "\n",
    "# train = np.nan_to_num(train)\n",
    "# test = np.nan_to_num(test)\n",
    "\n",
    "# X_val_scaled = X_train_scaled[hold_out:]\n",
    "# X_train_scaled = X_train_scaled[:hold_out]\n",
    "# y_val = train_y[hold_out:]\n",
    "# y_train = train_y[:hold_out]\n",
    "\n",
    "# val = train[hold_out:]\n",
    "# train = train[:hold_out]\n",
    "\n",
    "# X_train_scaled, train_y_t = torch.tensor(X_train_scaled.reshape(-1, 2, 200)).to(DEVICE), torch.tensor(train_y).to(DEVICE)  \n",
    "# X_test_scaled = torch.tensor(X_train_scaled.reshape(-1, 2, 200)).to(DEVICE)\n",
    "train_y_t = torch.tensor(train_y, dtype=torch.float32)\n",
    "X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_scaled = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "input_shape = X_train_scaled.shape[1]\n",
    "test_len = len(test_x)\n",
    "\n",
    "N_REPEAT = 5\n",
    "N_SKFOLD = 7\n",
    "N_EPOCH = 1000\n",
    "BATCH_SIZE = 2048\n",
    "VIRTUAL_BATCH_SIZE = 1024\n",
    "LOADER_PARAM = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True\n",
    "}\n",
    "\n",
    "prediction = np.zeros((test_len, 1), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_list = ['Q1','Q2','Q3','Q4','Q5', 'Q6','Q7','Q8','Q9',\n",
    "#             'Q10','Q11','Q12','Q13','Q14','Q15', 'Q16','Q17','Q18','Q19',\n",
    "#             'Q20','Q21','Q22','Q23','Q24','Q25', 'Q26',\n",
    "#             'index', 'country', 'introelapse','testelapse','surveyelapse']\n",
    "drop_list = ['index', 'country', ]\n",
    "# replace_dict = {'education': str, 'married': str}\n",
    "replace_dict = {'education': str, 'engnat': str, 'married': str, 'urban': str}\n",
    "# replace_dict = {'education': str, 'engnat': str, 'urban': str}\n",
    "train_data =pd.read_csv(path+'train.csv')\n",
    "hold_out = int(len(train_data)*0.8)\n",
    "test_data = pd.read_csv(path+'test.csv')\n",
    "\n",
    "# train_data = train_data.drop(train_data[train_data.familysize > 11].index)\n",
    "\n",
    "# train = train_data.drop(drop_list, axis=1)\n",
    "train_y = train_data['nerdiness']\n",
    "train_x = train_data.drop(drop_list + ['nerdiness'], axis=1)\n",
    "test_x = test_data.drop(drop_list, axis=1)\n",
    "\n",
    "train_x = train_x.reset_index(drop=True)\n",
    "train_y = train_y.reset_index(drop=True)\n",
    "\n",
    "\n",
    "columns = train_x.columns.values\n",
    "print(columns)\n",
    "\n",
    "train_x = pd.get_dummies(train_x)\n",
    "test_x = pd.get_dummies(test_x)\n",
    "\n",
    "imp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp_mode.fit(train_x)\n",
    "train_x = imp_mode.transform(train_x)\n",
    "test_x = imp_mode.transform(test_x)\n",
    "\n",
    "# scaler = RobustScaler()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(train_x)\n",
    "X_test_scaled = scaler.transform(test_x)\n",
    "\n",
    "# scaler.fit(train_x)\n",
    "# X_train_scaled = scaler.transform(train_x)\n",
    "# X_test_scaled = scaler.transform(test_x)\n",
    "\n",
    "# train = np.nan_to_num(train)\n",
    "# test = np.nan_to_num(test)\n",
    "\n",
    "# X_val_scaled = X_train_scaled[hold_out:]\n",
    "# X_train_scaled = X_train_scaled[:hold_out]\n",
    "# y_val = train_y[hold_out:]\n",
    "# y_train = train_y[:hold_out]\n",
    "\n",
    "# val = train[hold_out:]\n",
    "# train = train[:hold_out]\n",
    "\n",
    "# X_train_scaled, train_y_t = torch.tensor(X_train_scaled.reshape(-1, 2, 200)).to(DEVICE), torch.tensor(train_y).to(DEVICE)  \n",
    "# X_test_scaled = torch.tensor(X_train_scaled.reshape(-1, 2, 200)).to(DEVICE)\n",
    "pca = PCA(n_components=62) # 주성분을 몇개로 할지 결정\n",
    "X_train_scaled_printcipalComponents = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled_printcipalComponents = pca.transform(X_test_scaled)\n",
    "# principalDf = pd.DataFrame(data=printcipalComponents, columns = columns[0:62])\n",
    "print(X_train_scaled_printcipalComponents.shape)\n",
    "\n",
    "train_y_t = torch.tensor(train_y, dtype=torch.float32)\n",
    "X_train_scaled = torch.tensor(X_train_scaled_printcipalComponents, dtype=torch.float32)\n",
    "X_test_scaled = torch.tensor(X_test_scaled_printcipalComponents, dtype=torch.float32)\n",
    "\n",
    "test_len = len(test_x)\n",
    "\n",
    "N_REPEAT = 10\n",
    "N_SKFOLD = 7\n",
    "N_EPOCH = 100\n",
    "BATCH_SIZE = 2048\n",
    "VIRTUAL_BATCH_SIZE = 1024\n",
    "LOADER_PARAM = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True\n",
    "}\n",
    "\n",
    "prediction = np.zeros((test_len, 1), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/nawidsayed/lightgbm-and-cnn-3rd-place-solution/notebook\n",
    "# https://www.kaggle.com/code/nagiss/9-solution-nagiss-part-2-2-weight-sharing-nn/notebook\n",
    "\n",
    "# class Conv1d_with_SEBlock(nn.Module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__()\n",
    "#         pass\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, c, r=4):\n",
    "        super().__init__()\n",
    "\n",
    "        bottleneck = c // r\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Conv1d(c, bottleneck, kernel_size=1),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv1d(bottleneck, c, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, _= x.shape\n",
    "        y = self.squeeze(x).view(bs, c)\n",
    "        # excitation\n",
    "        y = y.unsqueeze(-1)\n",
    "        y = self.excitation(y).view(bs, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super(Conv1dBlock, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.swish = nn.SiLU()\n",
    "        # self.swish = nn.SiLU(inplace=True)\n",
    "        self.batchNorm1d = nn.BatchNorm1d(out_channels)\n",
    "        # self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1d(x)\n",
    "        x = self.batchNorm1d(x)\n",
    "        x = self.swish(x)\n",
    "        # x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class FcBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super(FcBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, out_channels)\n",
    "        \n",
    "        self.batchNorm1d1 = nn.BatchNorm1d(512)\n",
    "        self.batchNorm1d2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.swish = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(0.9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchNorm1d1(x)\n",
    "        x = self.swish(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchNorm1d2(x)\n",
    "        x = self.swish(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SangModel(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(SangModel, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.hidden_size = 64\n",
    "        \n",
    "        # self.conv11 = Conv1dBlock(63, self.hidden_size*1024) \n",
    "        # self.conv10 = Conv1dBlock(self.hidden_size*1024, self.hidden_size*512) \n",
    "        # self.conv9 = Conv1dBlock(self.hidden_size*512, self.hidden_size*256)  \n",
    "        # self.conv8 = Conv1dBlock(63, self.hidden_size*128)  \n",
    "        # self.conv7 = Conv1dBlock(63, self.hidden_size*64)  \n",
    "        self.conv6 = Conv1dBlock(self.input_shape, self.hidden_size*32)  \n",
    "        self.conv5 = Conv1dBlock(self.hidden_size*32, self.hidden_size*16)  \n",
    "        self.conv4 = Conv1dBlock(self.hidden_size*16, self.hidden_size*8)  \n",
    "        self.conv3 = Conv1dBlock(self.hidden_size*8, self.hidden_size*4)  \n",
    "        self.conv2 = Conv1dBlock(self.hidden_size*4, self.hidden_size*2)  \n",
    "        self.conv1 = Conv1dBlock(self.hidden_size*2, self.hidden_size)  \n",
    "\n",
    "        self.fc = FcBlock(self.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x_):\n",
    "        x = x_.unsqueeze(-1)\n",
    "        \n",
    "        # x = self.conv11(x)\n",
    "        # x = self.conv10(x)\n",
    "        # x = self.conv9(x)\n",
    "        # x = self.conv8(x)\n",
    "        # x = self.conv7(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x    \n",
    "\n",
    "# model = SangModel(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for repeat in range(N_REPEAT):\n",
    "\n",
    "    skf, tot = StratifiedKFold(n_splits=N_SKFOLD, random_state=repeat, shuffle=True), 0.\n",
    "    for skfold, (train_idx, valid_idx) in enumerate(skf.split(X_train_scaled, train_y_t)):\n",
    "        train_idx, valid_idx = list(train_idx), list(valid_idx)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(X_train_scaled[train_idx, :], train_y_t[train_idx]),\n",
    "                                  shuffle=True, drop_last=True, **LOADER_PARAM)\n",
    "        valid_loader = DataLoader(TensorDataset(X_train_scaled[valid_idx, :], train_y_t[valid_idx]),\n",
    "                                  shuffle=False, drop_last=False, **LOADER_PARAM)\n",
    "        test_loader = DataLoader(TensorDataset(X_test_scaled, torch.zeros((test_len,), dtype=torch.float32)),\n",
    "                                 shuffle=False, drop_last=False, **LOADER_PARAM)\n",
    "        \n",
    "        model = SangModel(input_shape)\n",
    "        # model.load_state_dict(torch.load(save_path+\"model_inverse_batchNorm_repeat0_fold0_0.7506608238347163.pth\")) # For overfitting. (lol)\n",
    "        model.to(DEVICE)\n",
    "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.20665], device=DEVICE))\n",
    "        # criterion = torch.nn.BCELoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=7.8e-2)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "        # scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            # optimizer, T_0=N_EPOCH // 6, eta_min=4e-6)\n",
    "        prediction_t, loss_t, acc_t = np.zeros((test_len, 1), dtype=np.float32), 1., 0.\n",
    "\n",
    "        # for epoch in range(N_EPOCH):\n",
    "        for epoch in tqdm(range(N_EPOCH), desc='{:02d}/{:02d}'.format(skfold + 1, N_SKFOLD)):\n",
    "            model.train()\n",
    "            train_running_acc, train_running_loss, train_running_count= 0, 0., 0\n",
    "            for idx, (xx, yy) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                xx, yy = xx.to(DEVICE), yy.to(DEVICE)\n",
    "                pred = model(xx).squeeze()\n",
    "                loss = criterion(pred, yy)\n",
    "                train_running_loss += loss.item() * len(yy)\n",
    "                train_running_count += len(yy)\n",
    "                train_running_acc += ((torch.sigmoid(pred) > 0.5).float() == yy).sum().item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"Train loss: {train_running_loss / train_running_count}\")\n",
    "                print(f\"Train accuracy: {train_running_acc / train_running_count}\")\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                running_acc, running_loss, running_count = 0, 0., 0\n",
    "                for xx, yy in valid_loader:\n",
    "                    xx, yy = xx.to(DEVICE), yy.to(DEVICE)\n",
    "                    pred = model(xx).squeeze()\n",
    "                    loss = criterion(pred, yy)\n",
    "                    running_loss += loss.item() * len(yy)\n",
    "                    running_count += len(yy)\n",
    "                    running_acc += ((torch.sigmoid(pred) > 0.5).float() == yy).sum().item()\n",
    "                # print('R{:02d} S{:02d} E{:02d} | {:6.4f}, {:5.2f}%'\n",
    "                #       .format(repeat + 1, skfold + 1, epoch + 1, running_loss / running_count,\n",
    "                #               running_acc / running_count * 100))\n",
    "                if epoch % 5 == 0:\n",
    "                    print(f\"Valid loss: {running_loss / running_count}\")\n",
    "                    print(f\"Valid accuracy: {running_acc / running_count}\")\n",
    "                # if running_acc / running_count > acc_t:\n",
    "                if running_loss / running_count < loss_t:\n",
    "                    loss_t = running_loss / running_count\n",
    "                    acc_t = running_acc / running_count\n",
    "                    torch.save(model.state_dict(), f\"{save_path}model_inverse_batchNorm_deeper64_repeat{repeat}_fold{skfold}_{loss_t}_acc{acc_t}.pth\")\n",
    "                    for idx, (xx, _) in enumerate(test_loader):\n",
    "                        xx = xx.to(DEVICE)\n",
    "                        # pred = (2. - torch.sigmoid(model(xx).detach().to('cpu'))).numpy()\n",
    "                        # pred = (torch.sigmoid(model(xx).detach().to('cpu'))).numpy()\n",
    "                        # pred = np.float32(pred > 0.5)\n",
    "                        pred = model(xx).detach().to('cpu').numpy()\n",
    "                        prediction_t[BATCH_SIZE * idx:min(BATCH_SIZE * (idx + 1), len(prediction)), :] \\\n",
    "                            = pred[:, :].copy()\n",
    "        prediction[:, :] += prediction_t[:, :].copy() / (N_REPEAT * N_SKFOLD)\n",
    "        tot += loss_t\n",
    "    print('R{} -> {:6.10f}'.format(repeat + 1, tot / N_SKFOLD))\n",
    "\n",
    "df = pd.read_csv(path+'sample_submission.csv')\n",
    "df.iloc[:, 1:] = prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using prediction in train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path+'model_inverse_batchNorm_sigmoid_repeat_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SangModel()\n",
    "model.load_state_dict(torch.load(save_path+\"model_inverse_deeper1_1_repeat0_fold2_3.762843291348371e-06.pth\"))\n",
    "# model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_scaled = X_test_scaled.to('cpu')\n",
    "# pred = model(X_test_scaled).detach().cpu().numpy()\n",
    "pred = model(X_test_scaled).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(path+'sample_submission.csv')\n",
    "submission[\"nerdiness\"] = pred\n",
    "submission.to_csv(\"submission_model_inverse_deeper1_1_repeat0_fold2_3.762843291348371e-06.csv\", index = False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1c3adb10a68128a1a8313746601dc1ecff251b3157a7a745306942cc936ba57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
